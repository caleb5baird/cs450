#+TITLE:
#+LANGUAGE: en
#+OPTIONS: H:4 num:nil toc:nil \n:nil @:t ::t |:t ^:t *:t TeX:t LaTeX:t d:(not "NOEXPORT")
#+OPTIONS: html-postamble:nil
#+OPTIONS: toc:3
#+STARTUP: showeverything entitiespretty
#+SETUPFILE: ~/.emacs.d/org-mode-themes/setup/theme-readtheorg.setup
#+HTML_HEAD: <style> </style>
**** Class:

     :NOEXPORT:
     [[elisp:(browse-url-of-file%20(org-html-export-to-html))][Export to HTML and Browse]]
     :END:

* Nueral Net
  #+Name: Neural_Net
  #+BEGIN_SRC python :noweb eval tangle :results silent
    import numpy as np
    import random;

    <<Node>>

    class Neural_Net():
      def __init__(self, hidden_layers):
        self.hidden_layers = list(map(lambda layer_size: np.empty(layer_size + 1, dtype=Node), hidden_layers))
        self.input_layer = np.array([], dtype=Node)
        self.output_layer = []
        for (layer_i, layer) in enumerate(self.hidden_layers):
          for node_i in range(0, len(self.hidden_layers[layer_i])):
            # Create a Perceptron object for each node in the hidden hidden_layers
            if node_i == 0: layer[node_i] = Bias_Node()
            else:layer[node_i] = Perceptron()
            # if ther is a next hidden layer set the weights going out of this node
            if layer_i + 1 < len(self.hidden_layers):
              # The "- 1" is to exlude the bias node
              layer[node_i].weights = np.random.rand(len(self.hidden_layers[layer_i + 1]) - 1)
              layer[node_i].weights = layer[node_i].weights * 2 - 1

      <<calc_activ>>

      <<load_inputs>>

      <<propagate_forward>>

      <<fit>>

      <<back_propagate>>

      <<adjust_weights>>

      <<predict_one>>

      <<predict>>

      <<train>>

      <<generate_dot_code>>
  #+END_SRC

** Calculate Activation
   #+Name:calc_activ
   #+BEGIN_SRC python :results silent :noweb eval tangle
     def calc_activ(self, prev_layer, index):
       """documentation"""
       import math
       h = sum(map(lambda node: node.value * node.weights[index], prev_layer))
       return 1/(1 + math.e **  -h)
   #+END_SRC

** Load Inputs
   #+Name:load_inputs
   #+BEGIN_SRC python :results silent :noweb eval tangle
     def load_inputs(self, data_vector):
       """documentation"""
       # Set Input Nodes (Add one to the input layer index to avoid the bias node)
       for (i, value) in enumerate(data_vector):
         self.input_layer[i + 1].value = value
   #+END_SRC

** Propagate Forward
   #+Name:propagate_forward
   #+BEGIN_SRC python :results silent :noweb eval tangle
     def propagate_forward(self):
       """documentation"""
       # Go through the hidden layers
       for (layer_i, layer) in enumerate(self.hidden_layers):
         prev_layer = self.input_layer if layer_i == 0 else self.hidden_layers[layer_i-1]
         for (node_i, node) in enumerate(layer[1:]): # Do not change Bias Nodes
           self.hidden_layers[layer_i][node_i + 1].value = self.calc_activ(prev_layer, node_i)

       # do the output layer
       for (node_i, node) in enumerate(self.output_layer):
         self.output_layer[node_i].value = self.calc_activ(self.hidden_layers[-1], node_i)
   #+END_SRC

** Fit
   #+Name:fit
   #+BEGIN_SRC python :results silent :noweb eval tangle
     def fit(self, data, targets, batch_size):
       """documentation"""
       # Create the input and output layers
       self.input_layer = np.empty(len(data[0]) + 1, dtype=Node)
       self.output_layer = np.empty(len(list(set(targets))), dtype=Output_Node)

       # Make the Input Nodes
       for node_i in range(0, len(self.input_layer)):
         self.input_layer[node_i] = Bias_Node() if node_i == 0 else Input_Node()
         # The "- 1" is to exlude the bias node
         self.input_layer[node_i].weights = np.random.rand(len(self.hidden_layers[0]) - 1)
         self.input_layer[node_i].weights = self.input_layer[node_i].weights * 2 - 1

       # Make the Output Nodes
       for (index, value) in enumerate(set(targets)):
         self.output_layer[index] = Output_Node(value)

       # Fill in the weights for the last hidden layer
       for i in range(0, len(self.hidden_layers[-1])):
         self.hidden_layers[-1][i].weights = np.random.rand(len(self.output_layer))
         self.hidden_layers[-1][i].weights = self.hidden_layers[-1][i].weights * 2 - 1

       # Train the NeuralNet
       # self.train(data, targets, batch_size)
   #+END_SRC

** Train
   #+Name:train
   #+BEGIN_SRC python :results silent :noweb eval tangle
     def train (self, data, targets, batch_size):
       """documentation"""

       self.layer_costs = list(map(lambda layer : np.full(len(layer), 0.0), self.hidden_layers))
       self.layer_costs.append(np.full(len(self.output_layer), 0.0))

       for i in range(0, batch_size):
        for ( index, data_vector ) in enumerate(data):
          if index < 5:
            target = targets[index]

            # make the target_vector
            target_vector = np.full(len(self.output_layer), 0)
            for (i, node) in enumerate(self.output_layer):
              if node.label == target:
                target_vector[i] = 1
                break

            self.predict_one(data_vector)
            output_vector = np.array(list(map(lambda node: node.value, self.output_layer)))
            self.layer_costs[-1] = output_vector * (1 - output_vector) * (output_vector - target_vector)

            self.back_propagate()
            self.adjust_weights()
            print('Index: ', index)

          # if (index + 1) % batch_size == 0:
   #+END_SRC

** Predict
   #+Name:predict
   #+BEGIN_SRC python :results silent :noweb eval tangle
     def predict(self, data):
       """documentation"""
       predictions = []
       for ( index, data_vector ) in enumerate(data):
         break
         # predictions.append(self.predict_one(data_vector))
       return predictions
   #+END_SRC

** Predict One
   #+Name:predict_one
   #+BEGIN_SRC python :results silent :noweb eval tangle
     def predict_one(self, data_vector):
       """documentation"""
       self.load_inputs(data_vector)
       print('data_vector: ', data_vector)
       input_layer = []
       for node in self.input_layer:
         input_layer.append(node.value)
       print('Input Layer', input_layer)
       self.propagate_forward()

       def get_node_value(node):
         return node.value

       output_layer = []
       for node in self.output_layer:
         output_layer.append(node.value)
       print('Output Layer', output_layer)
       return max(self.output_layer, key=get_node_value).label
   #+END_SRC
** Back Propagate
   #+Name:back_propagate
   #+BEGIN_SRC python :results silent :noweb eval tangle
     def back_propagate(self):
       """documentation"""
       for layer_i in range(len(self.hidden_layers) - 1, -1, -1):
         layers_a = np.array(list(map(lambda node: node.value, self.hidden_layers[layer_i])))
         weight_cost_sum = []
         for (node_index, node) in enumerate(self.hidden_layers[layer_i]):
           weight_cost_sum.append(
             sum(map(lambda weight, cost: weight * cost,
                     node.weights, self.layer_costs[layer_i + 1])))
         self.layer_costs[layer_i] = layers_a  * (1 - layers_a) * weight_cost_sum
   #+END_SRC
** Adjust Weights
   #+Name:adjust_weights
   #+BEGIN_SRC python :results silent :noweb eval tangle
     def adjust_weights(self):
       """documentation"""
       # Adjust input node weights
       for (n_i, node) in enumerate(self.input_layer):
         for (w_i, weight) in enumerate(node.weights):
           self.input_layer[n_i].weights[w_i] = weight - node.value * self.layer_costs[0][w_i]

       # Adjust hidden_layer node weights
       for (l_i, layer) in enumerate(self.hidden_layers):
        for (n_i, node) in enumerate(layer):
          for (w_i, weight) in enumerate(node.weights):
            self.hidden_layers[l_i][n_i].weights[w_i] = weight - node.value * self.layer_costs[1 + l_i][w_i]
   #+END_SRC
** Generate Dot Code
  #+Name:generate_dot_code
  #+BEGIN_SRC python :results silent :noweb eval tangle
    <<display_net>>

    def generate_dot_code (self):
      print('#+begin_src dot :file neural-net.png\n  graph decisionTree {\n    bgcolor = white;')
      self.display_net()
      print('}\n#+END_SRC')
  #+END_SRC
*** Display Net
  #+Name:display_net
  #+BEGIN_SRC python :results silent :noweb eval tangle
    def display_net (self):
      # Display input nodes and links
      for (i, node) in enumerate(self.input_layer):
        node_prefix = "Bias" if i == 0 else "Input"
        node_shape = "Mcircle" if i == 0 else "octagon"
        node_id = "{}_{}".format(node_prefix, i)
        node_name = "-1" if i == 0 else "{} {}\na = {}\n".format(node_prefix, i, node.value)
        print('    "{}"[shape="{}"][label="{}"]'.format(node_id, node_shape, node_name))
        for (j, j_node) in enumerate(self.hidden_layers[0]):
          if j != 0:
            print('    "{}" -- "HL_0_{}" [label="{:.3}"]'.format(node_id, j, node.weights[j - 1]))

      # Display output_nodes nodes
      for (i, node) in enumerate(self.output_layer):
        print('    "Out_{}"[shape="doubleoctagon"][label="{}\na = {:.3}\n {:.3}"]'.format(i, node.label, node.value, self.layer_costs[-1][i]))

      # Display the hidden nodes and links
      for (layer_i, layer) in enumerate(self.hidden_layers):
        for (i, node) in enumerate(layer):
          node_prefix = "Bias" if i == 0 else "HL"
          node_shape = "Mcircle" if i == 0 else "oval"
          node_id = "{}_{}_{}".format(node_prefix, layer_i, i)
          node_name = "-1" if i == 0 else "{} {}\na = {:.3}\nd = {:.3}".format(node_prefix, i, node.value, self.layer_costs[layer_i][i])
          print('    "{}"[shape="{}"][label="{}"]'.format(node_id, node_shape, node_name))
          # Connect it to the next layer
          if layer_i == len(self.hidden_layers) -1:
            for (j, j_node) in enumerate(self.output_layer):
              print('    "{}" -- "Out_{}" [label="{:.3}"]'.format(node_id, j, node.weights[j]))
          else:
            for j in range(1, len(self.hidden_layers[layer_i + 1])):
                jnode_id = "HL_{}_{}".format(layer_i+1, j)
                print('    "{}" -- "{}" [label="{:.3}"]'.format(node_id, jnode_id, node.weights[j-1]))

  #+END_SRC
** Node
  #+Name: Node
  #+BEGIN_SRC python :noweb eval tangle :results output
    class Node():
      def __init__(self, value):
        self.value = value

    <<Output_Node>>

    <<Input_Node>>

    <<Bias_Node>>

    <<Perceptron>>

  #+END_SRC

*** Output Node
    #+Name: Output_Node
    #+BEGIN_SRC python :noweb eval tangle :results output
      class Output_Node(Node):
        def __init__(self, label, activ = 0, delta = 0):
          super().__init__(activ)
          self.delta = delta
          self.label = label
          self.weights = np.array([], dtype = float)
        pass
    #+END_SRC

*** Input Node
    #+Name: Input_Node
    #+BEGIN_SRC python :noweb eval tangle :results silent
      class Input_Node(Node):
        def __init__(self, value = 0):
          super().__init__(value)
          self.weights = np.array([], dtype = float)
    #+END_SRC

*** Bias Node
    #+Name: Bias_Node
    #+BEGIN_SRC python :noweb eval tangle :results output
      class Bias_Node(Node):
        def __init__(self):
          super().__init__(-1)
          self.weights = np.array([], dtype = float)
    #+END_SRC

*** Perceptron
    #+Name: Perceptron
    #+BEGIN_SRC python :noweb eval tangle :results output
      class Perceptron(Node):
        def __init__(self, activ = 0, delta = 0):
          super().__init__(activ)
          self.delta = delta
          self.weights = np.array([], dtype = float)
    #+END_SRC

* Main
  #+BEGIN_SRC python :noweb eval tangle :results output :tangle mnist.py
    <<Neural_Net>>
    import mnist

    mndata = MNIST('samples')

    # images_train, labels_train = mndata.load_training()
    # index = 1
    # # print(mndata.display(images_train[index]))

    # nn = Neural_Net([20,20])

    # nn.fit(images_train[:100], labels_train[:100], 100)

    # images_test, labels_test = mndata.load_testing()
    # predictions = nn.predict(images_test)
    # right = 0
    # for predicted, target in zip(predictions, labels_test):
    #   if predicted == target: right += 1
    #   if predicted == 'ERROR': print(predicted, target)
    # accuracy = round(right / len(labels_test) * 100, 2)

    # print('My classifier was, {}% accurate'.format(accuracy))
  #+END_SRC

  #+results:

  #+Name: iris
  #+BEGIN_SRC python :noweb eval tangle :results output :tangle iris.py
    <<Neural_Net>>
    from sklearn import datasets
    import math

    nn = Neural_Net([5,5,5])
    iris = datasets.load_iris()

    length = len(iris.data)
    split_index = math.floor(length * .7)
    combined_data = np.array(list(map(lambda data_row, target: np.append(data_row, target),
                                      iris.data, iris.target)))
    np.random.shuffle(combined_data)

    train = combined_data[:split_index]
    test = combined_data[split_index:]
    data_train = train[:,:-1]
    targets_train = train[:,-1].astype(int)
    data_test = test[:,:-1]
    targets_test = test[:,-1].astype(int)
    # print(targets_train)

    trainings = 1
    nn.fit(data_train, targets_train, trainings)
    nn.train(data_train, targets_train, trainings)
    predictions = nn.predict(data_test)
    # print(predictions)
    # print(targets_test)
    # nn.generate_dot_code()


    right = 0
    for predicted, target in zip(predictions, targets_test):
      if predicted == target: right += 1
    accuracy = round(right / len(targets_test) * 100, 2)

    # for node in nn.hidden_layers[0]:
    #   print(node.value, node.weights)
    print(train)

    print('My classifier was, {}% accurate'.format(accuracy))

  #+END_SRC

  #+results: iris
  #+begin_example
  data_vector:  [4.6 3.2 1.4 0.2]
  Input Layer [-1, 4.6, 3.2, 1.4, 0.2]
  Output Layer [0.6566921815387798, 0.4101271779352742, 0.8461881341445597]
  Index:  0
  data_vector:  [6.  2.7 5.1 1.6]
  Input Layer [-1, 6.0, 2.7, 5.1, 1.6]
  Output Layer [0.6860250429100618, 0.3721937421758709, 0.8320456090443172]
  Index:  1
  data_vector:  [6.9 3.2 5.7 2.3]
  Input Layer [-1, 6.9, 3.2, 5.7, 2.3]
  Output Layer [0.6176731168912936, 0.42503612979380734, 0.7272676049553283]
  Index:  2
  data_vector:  [6.3 2.9 5.6 1.8]
  Input Layer [-1, 6.3, 2.9, 5.6, 1.8]
  Output Layer [0.5551689615840311, 0.38038111870121516, 0.759222016265605]
  Index:  3
  data_vector:  [5.5 2.4 3.7 1. ]
  Input Layer [-1, 5.5, 2.4, 3.7, 1.0]
  Output Layer [0.5022422152934195, 0.3514612141830675, 0.7821822971198579]
  Index:  4
  [[4.6 3.2 1.4 0.2 0. ]
   [6.  2.7 5.1 1.6 1. ]
   [6.9 3.2 5.7 2.3 2. ]
   [6.3 2.9 5.6 1.8 2. ]
   [5.5 2.4 3.7 1.  1. ]
   [6.2 3.4 5.4 2.3 2. ]
   [5.1 3.8 1.9 0.4 0. ]
   [4.4 2.9 1.4 0.2 0. ]
   [6.3 2.3 4.4 1.3 1. ]
   [5.7 2.9 4.2 1.3 1. ]
   [5.9 3.  5.1 1.8 2. ]
   [6.5 3.  5.8 2.2 2. ]
   [6.1 2.8 4.  1.3 1. ]
   [6.7 3.  5.  1.7 1. ]
   [6.4 2.8 5.6 2.1 2. ]
   [5.2 2.7 3.9 1.4 1. ]
   [6.4 2.8 5.6 2.2 2. ]
   [7.2 3.  5.8 1.6 2. ]
   [5.1 2.5 3.  1.1 1. ]
   [6.5 3.  5.5 1.8 2. ]
   [6.9 3.1 5.1 2.3 2. ]
   [6.7 3.3 5.7 2.1 2. ]
   [6.  3.4 4.5 1.6 1. ]
   [5.4 3.  4.5 1.5 1. ]
   [7.9 3.8 6.4 2.  2. ]
   [7.4 2.8 6.1 1.9 2. ]
   [6.5 3.2 5.1 2.  2. ]
   [6.7 3.1 5.6 2.4 2. ]
   [4.7 3.2 1.3 0.2 0. ]
   [6.2 2.8 4.8 1.8 2. ]
   [7.7 3.8 6.7 2.2 2. ]
   [6.8 2.8 4.8 1.4 1. ]
   [4.3 3.  1.1 0.1 0. ]
   [6.3 3.3 6.  2.5 2. ]
   [5.5 2.3 4.  1.3 1. ]
   [5.  3.4 1.6 0.4 0. ]
   [6.8 3.2 5.9 2.3 2. ]
   [4.9 2.4 3.3 1.  1. ]
   [5.  2.  3.5 1.  1. ]
   [5.8 2.7 4.1 1.  1. ]
   [7.3 2.9 6.3 1.8 2. ]
   [6.5 2.8 4.6 1.5 1. ]
   [5.1 3.5 1.4 0.3 0. ]
   [5.7 2.8 4.5 1.3 1. ]
   [5.8 2.7 5.1 1.9 2. ]
   [6.9 3.1 4.9 1.5 1. ]
   [5.5 4.2 1.4 0.2 0. ]
   [6.8 3.  5.5 2.1 2. ]
   [6.1 3.  4.9 1.8 2. ]
   [7.6 3.  6.6 2.1 2. ]
   [5.9 3.  4.2 1.5 1. ]
   [7.2 3.6 6.1 2.5 2. ]
   [5.4 3.9 1.7 0.4 0. ]
   [4.9 3.1 1.5 0.1 0. ]
   [5.2 4.1 1.5 0.1 0. ]
   [4.9 3.1 1.5 0.2 0. ]
   [6.3 2.5 5.  1.9 2. ]
   [6.7 3.3 5.7 2.5 2. ]
   [6.4 2.7 5.3 1.9 2. ]
   [4.8 3.  1.4 0.1 0. ]
   [4.4 3.2 1.3 0.2 0. ]
   [5.5 2.5 4.  1.3 1. ]
   [5.  2.3 3.3 1.  1. ]
   [6.6 3.  4.4 1.4 1. ]
   [5.  3.2 1.2 0.2 0. ]
   [5.4 3.4 1.7 0.2 0. ]
   [5.8 2.7 5.1 1.9 2. ]
   [5.6 3.  4.1 1.3 1. ]
   [5.6 2.9 3.6 1.3 1. ]
   [5.7 3.  4.2 1.2 1. ]
   [5.1 3.8 1.5 0.3 0. ]
   [6.4 3.2 5.3 2.3 2. ]
   [6.  3.  4.8 1.8 2. ]
   [4.9 3.6 1.4 0.1 0. ]
   [6.1 2.8 4.7 1.2 1. ]
   [4.5 2.3 1.3 0.3 0. ]
   [5.1 3.8 1.6 0.2 0. ]
   [6.4 3.2 4.5 1.5 1. ]
   [4.8 3.1 1.6 0.2 0. ]
   [6.3 2.5 4.9 1.5 1. ]
   [5.4 3.4 1.5 0.4 0. ]
   [7.7 3.  6.1 2.3 2. ]
   [5.1 3.4 1.5 0.2 0. ]
   [6.  2.2 4.  1.  1. ]
   [6.7 3.1 4.7 1.5 1. ]
   [5.  3.  1.6 0.2 0. ]
   [5.8 2.6 4.  1.2 1. ]
   [6.  2.2 5.  1.5 2. ]
   [6.7 3.1 4.4 1.4 1. ]
   [5.9 3.2 4.8 1.8 1. ]
   [6.3 3.3 4.7 1.6 1. ]
   [5.6 3.  4.5 1.5 1. ]
   [7.  3.2 4.7 1.4 1. ]
   [4.8 3.4 1.9 0.2 0. ]
   [6.3 3.4 5.6 2.4 2. ]
   [5.1 3.5 1.4 0.2 0. ]
   [5.  3.3 1.4 0.2 0. ]
   [5.  3.4 1.5 0.2 0. ]
   [5.6 2.8 4.9 2.  2. ]
   [5.7 4.4 1.5 0.4 0. ]
   [5.6 2.5 3.9 1.1 1. ]
   [5.2 3.4 1.4 0.2 0. ]
   [7.7 2.8 6.7 2.  2. ]
   [5.  3.5 1.3 0.3 0. ]
   [5.6 2.7 4.2 1.3 1. ]]
  My classifier was, 0.0% accurate
  #+end_example

  #+begin_src dot :file neural-net.png
    graph decisionTree {
      bgcolor = white;
      "Bias_0"[shape="Mcircle"][label="-1"]
      "Bias_0" -- "HL_0_1" [label="-7.83e+45"]
      "Bias_0" -- "HL_0_2" [label="-38.4"]
      "Bias_0" -- "HL_0_3" [label="-5.39e+11"]
      "Bias_0" -- "HL_0_4" [label="-6.55e+03"]
      "Bias_0" -- "HL_0_5" [label="-2.26"]
      "Input_1"[shape="octagon"][label="Input 1
  a = 5.7
  "]
      "Input_1" -- "HL_0_1" [label="4.59e+46"]
      "Input_1" -- "HL_0_2" [label="2.27e+02"]
      "Input_1" -- "HL_0_3" [label="3.15e+12"]
      "Input_1" -- "HL_0_4" [label="3.83e+04"]
      "Input_1" -- "HL_0_5" [label="14.3"]
      "Input_2"[shape="octagon"][label="Input 2
  a = 2.8
  "]
      "Input_2" -- "HL_0_1" [label="2.39e+46"]
      "Input_2" -- "HL_0_2" [label="1.17e+02"]
      "Input_2" -- "HL_0_3" [label="1.64e+12"]
      "Input_2" -- "HL_0_4" [label="1.99e+04"]
      "Input_2" -- "HL_0_5" [label="6.58"]
      "Input_3"[shape="octagon"][label="Input 3
  a = 4.5
  "]
      "Input_3" -- "HL_0_1" [label="2.97e+46"]
      "Input_3" -- "HL_0_2" [label="1.47e+02"]
      "Input_3" -- "HL_0_3" [label="2.04e+12"]
      "Input_3" -- "HL_0_4" [label="2.48e+04"]
      "Input_3" -- "HL_0_5" [label="8.85"]
      "Input_4"[shape="octagon"][label="Input 4
  a = 1.3
  "]
      "Input_4" -- "HL_0_1" [label="9.51e+45"]
      "Input_4" -- "HL_0_2" [label="46.2"]
      "Input_4" -- "HL_0_3" [label="6.56e+11"]
      "Input_4" -- "HL_0_4" [label="7.98e+03"]
      "Input_4" -- "HL_0_5" [label="2.97"]
      "Out_0"[shape="doubleoctagon"][label="0.0
  a = 0.0
   0.106"]
      "Out_1"[shape="doubleoctagon"][label="1.0
  a = 0.0
   0.325"]
      "Out_2"[shape="doubleoctagon"][label="2.0
  a = 0.0
   0.0584"]
      "Bias_0_0"[shape="Mcircle"][label="-1"]
      "Bias_0_0" -- "HL_1_1" [label="-5.29e+21"]
      "Bias_0_0" -- "HL_1_2" [label="-1.22e+03"]
      "Bias_0_0" -- "HL_1_3" [label="-4.55"]
      "Bias_0_0" -- "HL_1_4" [label="-3.67e+16"]
      "Bias_0_0" -- "HL_1_5" [label="-1.97e+03"]
      "HL_0_1"[shape="oval"][label="HL 1
  a = 1.0
  d = -0.00921"]
      "HL_0_1" -- "HL_1_1" [label="5.29e+21"]
      "HL_0_1" -- "HL_1_2" [label="1.22e+03"]
      "HL_0_1" -- "HL_1_3" [label="5.16"]
      "HL_0_1" -- "HL_1_4" [label="3.67e+16"]
      "HL_0_1" -- "HL_1_5" [label="1.97e+03"]
      "HL_0_2"[shape="oval"][label="HL 2
  a = 1.0
  d = -1.3e+08"]
      "HL_0_2" -- "HL_1_1" [label="5.29e+21"]
      "HL_0_2" -- "HL_1_2" [label="1.21e+03"]
      "HL_0_2" -- "HL_1_3" [label="4.77"]
      "HL_0_2" -- "HL_1_4" [label="3.67e+16"]
      "HL_0_2" -- "HL_1_5" [label="1.97e+03"]
      "HL_0_3"[shape="oval"][label="HL 3
  a = 1.0
  d = -1.56"]
      "HL_0_3" -- "HL_1_1" [label="5.29e+21"]
      "HL_0_3" -- "HL_1_2" [label="1.22e+03"]
      "HL_0_3" -- "HL_1_3" [label="5.72"]
      "HL_0_3" -- "HL_1_4" [label="3.67e+16"]
      "HL_0_3" -- "HL_1_5" [label="1.97e+03"]
      "HL_0_4"[shape="oval"][label="HL 4
  a = 1.0
  d = -0.000562"]
      "HL_0_4" -- "HL_1_1" [label="5.29e+21"]
      "HL_0_4" -- "HL_1_2" [label="1.21e+03"]
      "HL_0_4" -- "HL_1_3" [label="5.45"]
      "HL_0_4" -- "HL_1_4" [label="3.67e+16"]
      "HL_0_4" -- "HL_1_5" [label="1.97e+03"]
      "HL_0_5"[shape="oval"][label="HL 5
  a = 1.0
  d = -1.45e+25"]
      "HL_0_5" -- "HL_1_1" [label="5.29e+21"]
      "HL_0_5" -- "HL_1_2" [label="1.21e+03"]
      "HL_0_5" -- "HL_1_3" [label="6.31"]
      "HL_0_5" -- "HL_1_4" [label="3.67e+16"]
      "HL_0_5" -- "HL_1_5" [label="1.96e+03"]
      "Bias_1_0"[shape="Mcircle"][label="-1"]
      "Bias_1_0" -- "HL_2_1" [label="-2.97e+09"]
      "Bias_1_0" -- "HL_2_2" [label="-3.44e+02"]
      "Bias_1_0" -- "HL_2_3" [label="1.25e+02"]
      "Bias_1_0" -- "HL_2_4" [label="-4.34e+03"]
      "Bias_1_0" -- "HL_2_5" [label="-2.49e+03"]
      "HL_1_1"[shape="oval"][label="HL 1
  a = 1.0
  d = -0.289"]
      "HL_1_1" -- "HL_2_1" [label="2.97e+09"]
      "HL_1_1" -- "HL_2_2" [label="3.44e+02"]
      "HL_1_1" -- "HL_2_3" [label="-1.24e+02"]
      "HL_1_1" -- "HL_2_4" [label="4.34e+03"]
      "HL_1_1" -- "HL_2_5" [label="2.49e+03"]
      "HL_1_2"[shape="oval"][label="HL 2
  a = 1.0
  d = -0.00135"]
      "HL_1_2" -- "HL_2_1" [label="2.97e+09"]
      "HL_1_2" -- "HL_2_2" [label="3.44e+02"]
      "HL_1_2" -- "HL_2_3" [label="-1.24e+02"]
      "HL_1_2" -- "HL_2_4" [label="4.34e+03"]
      "HL_1_2" -- "HL_2_5" [label="2.49e+03"]
      "HL_1_3"[shape="oval"][label="HL 3
  a = 1.0
  d = -1.09e+13"]
      "HL_1_3" -- "HL_2_1" [label="2.97e+09"]
      "HL_1_3" -- "HL_2_2" [label="3.23e+02"]
      "HL_1_3" -- "HL_2_3" [label="-1.16e+02"]
      "HL_1_3" -- "HL_2_4" [label="4.08e+03"]
      "HL_1_3" -- "HL_2_5" [label="2.34e+03"]
      "HL_1_4"[shape="oval"][label="HL 4
  a = 1.0
  d = -0.47"]
      "HL_1_4" -- "HL_2_1" [label="2.97e+09"]
      "HL_1_4" -- "HL_2_2" [label="3.43e+02"]
      "HL_1_4" -- "HL_2_3" [label="-1.23e+02"]
      "HL_1_4" -- "HL_2_4" [label="4.34e+03"]
      "HL_1_4" -- "HL_2_5" [label="2.49e+03"]
      "HL_1_5"[shape="oval"][label="HL 5
  a = 1.0
  d = -75.4"]
      "HL_1_5" -- "HL_2_1" [label="2.97e+09"]
      "HL_1_5" -- "HL_2_2" [label="3.44e+02"]
      "HL_1_5" -- "HL_2_3" [label="-1.23e+02"]
      "HL_1_5" -- "HL_2_4" [label="4.34e+03"]
      "HL_1_5" -- "HL_2_5" [label="2.49e+03"]
      "Bias_2_0"[shape="Mcircle"][label="-1"]
      "Bias_2_0" -- "Out_0" [label="4.46e+02"]
      "Bias_2_0" -- "Out_1" [label="1.37e+03"]
      "Bias_2_0" -- "Out_2" [label="2.45e+02"]
      "HL_2_1"[shape="oval"][label="HL 1
  a = 1.0
  d = -0.0819"]
      "HL_2_1" -- "Out_0" [label="-4.45e+02"]
      "HL_2_1" -- "Out_1" [label="-1.37e+03"]
      "HL_2_1" -- "Out_2" [label="-2.45e+02"]
      "HL_2_2"[shape="oval"][label="HL 2
  a = 1.0
  d = 0.0295"]
      "HL_2_2" -- "Out_0" [label="-4.44e+02"]
      "HL_2_2" -- "Out_1" [label="-1.36e+03"]
      "HL_2_2" -- "Out_2" [label="-2.45e+02"]
      "HL_2_3"[shape="oval"][label="HL 3
  a = 0.0
  d = -1.04"]
      "HL_2_3" -- "Out_0" [label="-0.525"]
      "HL_2_3" -- "Out_1" [label="-1.53"]
      "HL_2_3" -- "Out_2" [label="-0.74"]
      "HL_2_4"[shape="oval"][label="HL 4
  a = 1.0
  d = -0.595"]
      "HL_2_4" -- "Out_0" [label="-4.45e+02"]
      "HL_2_4" -- "Out_1" [label="-1.36e+03"]
      "HL_2_4" -- "Out_2" [label="-2.45e+02"]
      "HL_2_5"[shape="oval"][label="HL 5
  a = 1.0
  d = 0.0358"]
      "HL_2_5" -- "Out_0" [label="-4.44e+02"]
      "HL_2_5" -- "Out_1" [label="-1.37e+03"]
      "HL_2_5" -- "Out_2" [label="-2.45e+02"]
  }
  #+END_SRC

  #+results:
  [[file:neural-net.png]]

  #+BEGIN_SRC python :noweb eval tangle :results output :tangle test.py
    <<Neural_Net>>

    nn = Neural_Net([2,2])

    data = [[1.2, -.2], [1.2, -.2], [1.2, -.2], [1.2, -.2], [1.2, -.2], [1.2, -.2], [1.2, -.2], [1.2, -.2], [1.2, -.2], [1.2, -.2], [1.2, -.2], [1.2, -.2], [1.2, -.2], [1.2, -.2], [1.2, -.2]]
    targets = ['B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A' ]
    nn.fit(data, targets, 3)

    nn.input_layer[0].weights = np.array([.2, -.1])
    nn.input_layer[1].weights = np.array([.3, .2])
    nn.input_layer[2].weights = np.array([.4, -.1])
    nn.hidden_layers[0][0].weights = np.array([.3, .1])
    nn.hidden_layers[0][1].weights = np.array([.2, -.2])
    nn.hidden_layers[0][2].weights = np.array([-.1, .4])
    # nn.predict(data)
    nn.train(data, targets, 1)
    # output_vector = np.array(list(map(lambda node: node.value, nn.output_layer)))
    # print(output_vector)
    # print(nn.hidden_layers[0][0].value)
    nn.generate_dot_code()

    # print(list(map(lambda node: node.value, nn.output_layer)))
  #+END_SRC

  #+begin_src dot :file neural-net.png
    graph decisionTree {
      bgcolor = white;
      "Bias_0"[shape="Mcircle"][label="-1"]
      "Bias_0" -- "HL_0_1" [label="-6.75e+04"]
      "Bias_0" -- "HL_0_2" [label="-0.0713"]
      "Input_1"[shape="octagon"][label="Input 1
  a = 1.2
  "]
      "Input_1" -- "HL_0_1" [label="8.1e+04"]
      "Input_1" -- "HL_0_2" [label="0.166"]
      "Input_2"[shape="octagon"][label="Input 2
  a = -0.2
  "]
      "Input_2" -- "HL_0_1" [label="-1.35e+04"]
      "Input_2" -- "HL_0_2" [label="-0.0943"]
      "Out_0"[shape="doubleoctagon"][label="B
  a = 1.4e-05
   0.626"]
      "Out_1"[shape="doubleoctagon"][label="A
  a = 0.999
   -0.352"]
      "Bias_0_0"[shape="Mcircle"][label="-1"]
      "Bias_0_0" -- "HL_1_1" [label="-2.06e+02"]
      "Bias_0_0" -- "HL_1_2" [label="0.176"]
      "HL_0_1"[shape="oval"][label="HL 1
  a = 1.0
  d = -0.00127"]
      "HL_0_1" -- "HL_1_1" [label="2.06e+02"]
      "HL_0_1" -- "HL_1_2" [label="-0.253"]
      "HL_0_2"[shape="oval"][label="HL 2
  a = 0.571
  d = -2.42e+03"]
      "HL_0_2" -- "HL_1_1" [label="1.17e+02"]
      "HL_0_2" -- "HL_1_2" [label="0.356"]
      "Bias_1_0"[shape="Mcircle"][label="-1"]
      "Bias_1_0" -- "Out_0" [label="6.39"]
      "Bias_1_0" -- "Out_1" [label="-4.54"]
      "HL_1_1"[shape="oval"][label="HL 1
  a = 1.0
  d = -0.00404"]
      "HL_1_1" -- "Out_0" [label="-4.61"]
      "HL_1_1" -- "Out_1" [label="2.9"]
      "HL_1_2"[shape="oval"][label="HL 2
  a = 0.441
  d = -3.08"]
      "HL_1_2" -- "Out_0" [label="-3.49"]
      "HL_1_2" -- "Out_1" [label="0.717"]
  }
  #+END_SRC

  #+results:
  [[file:neural-net.png]]




* rest
  #+begin_src dot :file nn-test.png
  graph G {
    bgcolor = transparent;
    "i_Bias\n -1" -- "hl1_one" [label=".2"]
    "i_Bias\n -1" -- "hl1_two" [label="-.1"]
    "x1\n .8" -- "hl1_one" [label=".3"]
    "x1\n .8" -- "hl1_two" [label=".2"]
    "x2\n .1" -- "hl1_one" [label=".4"]
    "x2\n .1" -- "hl1_two" [label="-.1"]
    "hl1Bias\n -1" -- "out_1"
    "hl1Bias\n -1" -- "out_2"
    "hl1Bias\n -1" -- "out_3"
    "hl1_one" -- "out_1"
    "hl1_one" -- "out_2"
    "hl1_one" -- "out_3"
    "hl1_two" -- "out_1"
    "hl1_two" -- "out_2"
    "hl1_two" -- "out_3"
    "hl1_two"[shape="Mcircle"][label="TEST"]


  }
  #+end_src

  #+results:
  [[file:tree.png]]

#+BEGIN_SRC python :noweb eval tangle :results output :tangle debug.py
  # l = [1,2,3,4,5]
  # print(l[1:])
  # <<Node>>
  # i = Input_Node(5)
  # print(i.value.activ)
  l = [1,2,3,4]
  l.append(5)
  print(l)

      # <<Perceptron>>

      # ar1 = np.empty(5, dtype=Perceptron)
      # ar2 = np.random.rand(5)
      # # print(ar2)
      # for i in range(0, len(ar1)):
      #   ar1[i] = Perceptron()
      #   ar1[i].value.activ = i
      #   # print(ar1[i].value.activ)

      # print(sum(map(lambda x, y: x.value.activ * y, ar1, ar2)))
#+END_SRC

#+results:
: [1, 2, 3, 4, 5]
